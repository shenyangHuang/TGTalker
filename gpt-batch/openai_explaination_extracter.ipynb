{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eeda395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from tgb.linkproppred.evaluate import Evaluator\n",
    "from tgb.linkproppred.dataset import LinkPropPredDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1a425d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"uci\"\n",
    "dataset = f\"tgbl-{data}\"\n",
    "model = \"gpt-4o-mini-2024-07-18\"   #  \"gpt-4.1-mini-2025-04-14\"\n",
    "prompt_version = \"base_v1_cot\"\n",
    "\n",
    "result_folder = f\"./result/{dataset}/{model}/{prompt_version}/output\"\n",
    "\n",
    "output_folder = f\"./explanations/{model}\"\n",
    "output_file = f\"{output_folder}/explanations_{data}.csv\"\n",
    "\n",
    "folder = f\"./output/{dataset}/{model}/{prompt_version}\"\n",
    "answer_key = f\"{folder}/answer_key.parquet\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "52cb8b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw file found, skipping download\n",
      "Dataset directory is  /Users/zacharyyang/anaconda3/lib/python3.10/site-packages/tgb/datasets/tgbl_uci\n",
      "loading processed file\n"
     ]
    }
   ],
   "source": [
    "link_prop_dataset = LinkPropPredDataset(name=dataset, root=\"datasets\", preprocess=True)\n",
    "evaluator = Evaluator(name=dataset)\n",
    "metric = link_prop_dataset.eval_metric\n",
    "evaluator = Evaluator(name=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "302aaea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing JSON: Unterminated string starting at: line 1 column 12 (char 11)\n",
      "Error parsing JSON: Expecting ',' delimiter: line 42 column 216 (char 118373)\n",
      "Total errors: 2\n",
      "Total results: 8974\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "error_count = 0\n",
    "\n",
    "for file in glob(f\"{result_folder}/*.jsonl\"):\n",
    "    with open(file, \"r\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            id = data.get(\"custom_id\")\n",
    "            model_answer = data[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "            try:\n",
    "                explanation = []\n",
    "                \n",
    "                model_answer = json.loads(model_answer)\n",
    "                destination_node = model_answer[\"destination_node\"]\n",
    "                for step in model_answer[\"steps\"]:\n",
    "                    explanation.append(step[\"explanation\"])\n",
    "                explanation = \"\\n\".join(explanation)\n",
    "\n",
    "                result.append({\"task_id\": id, \n",
    "                               \"explanation\": explanation,\n",
    "                               \"destination_node\": destination_node})\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing JSON: {e}\")\n",
    "                error_count += 1\n",
    "                continue\n",
    "            \n",
    "print(f\"Total errors: {error_count}\")\n",
    "print(f\"Total results: {len(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eafac9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_df = pd.DataFrame(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "25757179",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_key = pd.read_parquet(answer_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0740bdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_df = explanation_df.merge(answer_key, on=\"task_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9bc6e816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_link(query_dst: np.ndarray, llm_dst: int) -> np.ndarray:\n",
    "    r\"\"\"\n",
    "    convert LLM prediction into MRR format, just check if the LLM prediction is within the possible destinations\n",
    "    \"\"\"\n",
    "    pred = np.zeros(len(query_dst))\n",
    "    idx = 0\n",
    "    for dst in query_dst:\n",
    "        if (dst == llm_dst):\n",
    "            pred[idx] = 1.0\n",
    "        idx += 1\n",
    "    return pred\n",
    "\n",
    "def get_score(query_dst: np.ndarray, \n",
    "              llm_dst: int,\n",
    "              evaluator,\n",
    "              metric: str) -> float:\n",
    "    r\"\"\"\n",
    "    get the score of the LLM prediction\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred = predict_link(query_dst, llm_dst)\n",
    "    input_dict = {\n",
    "            \"y_pred_pos\": np.array([y_pred[0]]),\n",
    "            \"y_pred_neg\": np.array(y_pred[1:]),\n",
    "            \"eval_metric\": [metric],\n",
    "        }\n",
    "    return evaluator.eval(input_dict)[metric]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b379d532",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_df[\"score\"] = explanation_df.apply(\n",
    "    lambda row: get_score(\n",
    "        row[\"query_dst\"][0],\n",
    "        row[\"destination_node\"],\n",
    "        evaluator,\n",
    "        metric\n",
    "    ),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5182c3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score for tgbl-uci base_v1_cot is: 0.057\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average Score for {dataset} {prompt_version} is: {np.mean(explanation_df['score']):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b9d028ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_df[[\"task_id\",\"destination_node\", \"score\", \"explanation\",]].to_csv(output_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
