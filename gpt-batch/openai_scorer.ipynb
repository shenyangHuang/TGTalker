{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eeda395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from tgb.linkproppred.evaluate import Evaluator\n",
    "from tgb.linkproppred.dataset import LinkPropPredDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a425d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = \"tgbl-wiki\"\n",
    "# model = \"gpt-4o-mini-2024-07-18\"\n",
    "# prompt_version = \"base_v1_icl\"\n",
    "\n",
    "dataset = \"tgbl-lastfm\"\n",
    "model = \"gpt-4.1-mini-2025-04-14\"\n",
    "prompt_version = \"base_v1_icl\"\n",
    "\n",
    "folder = f\"./output/{dataset}/{model}/{prompt_version}\"\n",
    "answer_key = f\"{folder}/answer_key.parquet\"\n",
    "\n",
    "result_folder = f\"./result/{dataset}/{model}/{prompt_version}/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c03ebd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw file found, skipping download\n",
      "Dataset directory is  /Users/zacharyyang/anaconda3/lib/python3.10/site-packages/tgb/datasets/tgbl_lastfm\n",
      "loading processed file\n"
     ]
    }
   ],
   "source": [
    "link_prop_dataset = LinkPropPredDataset(name=dataset, root=\"datasets\", preprocess=True)\n",
    "evaluator = Evaluator(name=dataset)\n",
    "metric = link_prop_dataset.eval_metric\n",
    "evaluator = Evaluator(name=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d0b5a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_key = pd.read_parquet(answer_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff4f6329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_destination_node(text: str) -> int:\n",
    "    \"\"\"Extract the destination node from the model answer.\"\"\"\n",
    "    pattern = r'[-+]?\\d*\\.\\d+|\\d+$'  # Matches decimal or integer at end of string\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "302aaea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error count: 0\n",
      "Empty count: 3\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "error_count = 0\n",
    "empty_count = 0\n",
    "\n",
    "for file in glob(f\"{result_folder}/*.jsonl\"):\n",
    "    with open(file, \"r\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                id = data.get(\"custom_id\")\n",
    "                \n",
    "                # Default value\n",
    "                destination_node = -1\n",
    "                    \n",
    "                model_answer = data[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "                if not model_answer:\n",
    "                    empty_count +=1\n",
    "                    result.append({\"task_id\": id, \"destination_node\": destination_node})\n",
    "                    continue\n",
    "                \n",
    "                # Clean the model answer\n",
    "                model_answer = str(model_answer).replace(\"\\n\", \"\").replace(\"\\r \", \"\").replace(\"\\t\", \"\")\n",
    "                \n",
    "                if model_answer.strip() == \"\":\n",
    "                    empty_count +=1\n",
    "                    result.append({\"task_id\": id, \"destination_node\": destination_node})\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    parsed_answer = json.loads(model_answer)\n",
    "                    destination_node = parsed_answer[\"destination_node\"]\n",
    "                except Exception as e:\n",
    "                    destination_node = extract_destination_node(model_answer)\n",
    "                    error_count += 1\n",
    "                    print(f\"Error parsing JSON: {model_answer}\")\n",
    "                            \n",
    "                result.append({\"task_id\": id, \"destination_node\": destination_node})\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing line: {line}\")\n",
    "                    \n",
    "\n",
    "                    \n",
    "result = pd.DataFrame(result)\n",
    "print(f\"Error count: {error_count}\")\n",
    "print(f\"Empty count: {empty_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a881f17e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(193966, 193966)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity Check\n",
    "len(result), len(answer_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d05ad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_key = answer_key.merge(result, on=\"task_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9de5b2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_link(query_dst: np.ndarray, llm_dst: int) -> np.ndarray:\n",
    "    r\"\"\"\n",
    "    convert LLM prediction into MRR format, just check if the LLM prediction is within the possible destinations\n",
    "    \"\"\"\n",
    "    pred = np.zeros(len(query_dst))\n",
    "    idx = 0\n",
    "    for dst in query_dst:\n",
    "        if (dst == llm_dst):\n",
    "            pred[idx] = 1.0\n",
    "        idx += 1\n",
    "    return pred\n",
    "\n",
    "def get_score(query_dst: np.ndarray, \n",
    "              llm_dst: int,\n",
    "              evaluator,\n",
    "              metric: str) -> float:\n",
    "    r\"\"\"\n",
    "    get the score of the LLM prediction\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred = predict_link(query_dst, llm_dst)\n",
    "    input_dict = {\n",
    "            \"y_pred_pos\": np.array([y_pred[0]]),\n",
    "            \"y_pred_neg\": np.array(y_pred[1:]),\n",
    "            \"eval_metric\": [metric],\n",
    "        }\n",
    "    return evaluator.eval(input_dict)[metric]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8849d636",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_key[\"score\"] = answer_key.apply(\n",
    "    lambda row: get_score(\n",
    "        row[\"query_dst\"][0],\n",
    "        row[\"destination_node\"],\n",
    "        evaluator,\n",
    "        metric\n",
    "    ),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84ae5287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score for tgbl-lastfm base_v1_icl is: 0.065\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average Score for {dataset} {prompt_version} is: {np.mean(answer_key['score']):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
